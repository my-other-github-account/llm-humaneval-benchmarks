{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba32bb-854a-4230-9cd1-2f509fa17bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from launch_pod import launch_webui, stop_and_terminate_pod, remove_safetensors, pod_benchmark, killall_pods\n",
    "from templates import *\n",
    "from multiprocessing import Process\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd2e0f-3cdf-417c-bf11-e7bdf09c5161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "API_KEY = 'YOUR KEY HERE'\n",
    "file_tag = \"whatever_you_want\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84085ea-ad32-41c5-aa04-fa2672583845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"TheBloke/wizardLM-7B-GPTQ\": {\"template\": \"ALPACA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A4000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\": {\"template\": \"VICUNA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A4000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/vicuna-7B-GPTQ-4bit-128g\": {\"template\": \"VICUNA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/guanaco-7B-GPTQ\": {\"template\": \"ALPACA/SHORT\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A4000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\": {\"template\": \"VICUNA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/wizardLM-13B-1.0-GPTQ\": {\"template\": \"ALPACA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/guanaco-13B-GPTQ\": {\"template\": \"ALPACA/SHORT\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/Manticore-13B-GPTQ\": {\"template\": \"ALPACA/VICUNA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"mindrage/Manticore-13B-Chat-Pyg-Guanaco-GPTQ-4bit-128g.no-act-order.safetensors\": {\"template\": \"ALPACA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\": {\"template\": \"VICUNA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/VicUnlocked-30B-LoRA-GPTQ\": {\"template\": \"ALPACA/VICUNA\", \"groupsize\": \"\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/WizardLM-30B-Uncensored-GPTQ\": {\"template\": \"ALPACA/VICUNA\", \"groupsize\": \"\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\": {\"template\": \"VICUNA\", \"groupsize\": \"\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/guanaco-33B-GPTQ\": {\"template\": \"ALPACA/SHORT\", \"groupsize\": \"\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/guanaco-65B-GPTQ\": {\"template\": \"ALPACA/SHORT\", \"groupsize\": \"\", \"wbits\": \"4\", \"GPU\": \"NVIDIA A100-SXM4-80GB\", \"additional_cmd\": \"\"},\n",
    "    \"Aeala/VicUnlocked-alpaca-65b-4bit\": {\"template\": \"ALPACA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"2xNVIDIA A100-SXM4-80GB\", \"additional_cmd\": \"\"},\n",
    "}\n",
    "additional_models = {\n",
    "    \"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\": {\"template\": \"Q_A\", \"groupsize\": \"\", \"wbits\": \"\", \"GPU\": \"NVIDIA RTX A4000\", \"additional_cmd\": \"\"},\n",
    "    \"togethercomputer/RedPajama-INCITE-7B-Instruct\": {\"template\": \"Q_A\", \"groupsize\": \"\", \"wbits\": \"\", \"GPU\": \"NVIDIA RTX A4000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/falcon-7b-instruct-GPTQ\": {\"template\": \"FALCON\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A4000\", \"additional_cmd\": \"\"},\n",
    "    \"anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\": {\"template\": \"ALPACA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/Nous-Hermes-13B-GPTQ\": {\"template\": \"ALPACA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/wizard-vicuna-13B-GPTQ\": {\"template\": \"VICUNA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct\": {\"template\": \"ALPACA_INPUT\", \"groupsize\": \"\", \"wbits\": \"\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"sahil2801/instruct-codegen-16B\": {\"template\": \"ALPACA_PREFIX1\", \"groupsize\": \"\", \"wbits\": \"\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\": {\"template\": \"OPENA\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/WizardLM-30B-GPTQ\": {\"template\": \"VICUNA\", \"groupsize\": \"\", \"wbits\": \"4\", \"GPU\": \"NVIDIA RTX A6000\", \"additional_cmd\": \"\"},\n",
    "    \"TheBloke/falcon-40b-instruct-GPTQ\": {\"template\": \"FALCON\", \"groupsize\": \"128\", \"wbits\": \"4\", \"GPU\": \"NVIDIA A100-SXM4-80GB\", \"additional_cmd\": \"\"},\n",
    "}\n",
    "\n",
    "combined_models = {**models, **additional_models}\n",
    "\n",
    "template_map = {\"ALPACA\" : ALPACA_TEMPLATE, \n",
    "                \"VICUNA\" : VICUNA_TEMPLATE,\n",
    "                \"ALPACA_INPUT\" : ALPACA_INPUT_TEMPLATE,\n",
    "                \"OPENA\" : OPENASSISTANT_TEMPLATE,\n",
    "                \"FALCON\" : FALCON_TEMPLATE,\n",
    "                \"Q_A\" : QA_TEMPLATE,\n",
    "                \"ALPACA_PREFIX1\" : ALPACA_PREFIX_TEMPLATE,\n",
    "                \"ALPACA_MEDIUM\" : ALPACA_TEMPLATE_MEDIUM, \n",
    "                \"ALPACA_INPUT_MEDIUM\" : ALPACA_INPUT_TEMPLATE_MEDIUM,\n",
    "                \"ALPACA_PREFIX1_MEDIUM\" : ALPACA_PREFIX_TEMPLATE_MEDIUM,\n",
    "                \"SHORT\" : SHORT_TEMPLATE,\n",
    "                \"VERYSHORT\" : VERYSHORT_TEMPLATE\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd10d9a-303c-4f18-a3c4-c66a252ec215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "def process_model(model, file_tag=\"\"):\n",
    "    model_file = model.replace(\"/\", \"_\")\n",
    "    pod_tag = model_file\n",
    "\n",
    "    if not os.path.exists(\"logs\"):\n",
    "        os.mkdir(\"logs\")\n",
    "\n",
    "    log_filename = f\"logs/log_{model_file}_{file_tag}.txt\"  # Define your log file name here\n",
    "    print(\"Starting\", model)\n",
    "    # Redirect standard output and error to the log file\n",
    "    with open(log_filename, 'w') as f, contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "        gpu_type = combined_models[model][\"GPU\"]\n",
    "        groupsize = combined_models[model][\"groupsize\"]\n",
    "        wbits = combined_models[model][\"wbits\"]\n",
    "        gptq_params = \"\"\n",
    "\n",
    "        if groupsize:\n",
    "            gptq_params += f\"--groupsize {groupsize} \"\n",
    "        if wbits:\n",
    "            gptq_params += f\"--wbits {wbits} \"\n",
    "\n",
    "        templ = combined_models[model][\"template\"]\n",
    "        if \"/\" in templ:\n",
    "            templ = templ.split(\"/\")[0]\n",
    "        prompt = template_map[templ]\n",
    "        print(pod_tag, gpu_type, gptq_params,\"*** PROMPT ***\", prompt,\"*** *** ***\", sep=\"\\n\")\n",
    "\n",
    "        api_url, model_file, pod_id = launch_webui(api_key=API_KEY, \n",
    "                               model=model, \n",
    "                               gptq_params=gptq_params, \n",
    "                               gpu_type=gpu_type,\n",
    "                               pod_num=pod_tag)\n",
    "\n",
    "\n",
    "        print(\"starting benchmark...\")\n",
    "        pod_benchmark(model_file + \"_\" + templ + \"_\" + file_tag, prompt, assistant_tag=prompt[-15:], pod_id=pod_id, api_key=API_KEY, start_from=0, host=api_url, port=443, insert_func_stub=True)\n",
    "\n",
    "        stop_and_terminate_pod(pod_id, API_KEY)\n",
    "\n",
    "        # Close the log files\n",
    "        sys.stdout.close()\n",
    "        sys.stderr.close()\n",
    "    print(\"Finished\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434f398-497c-4234-aa79-09807866c986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to_run = [f for f in sorted(combined_models.keys()) if \"7B\" in f][:4]\n",
    "to_run = combined_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd713b7-81ed-4c80-9d1c-759d1e539bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import contextlib\n",
    "import os\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def worker(model):\n",
    "    return process_model(model, file_tag)\n",
    "\n",
    "pool = Pool(processes=MAX_WORKERS)\n",
    "\n",
    "for model in to_run:\n",
    "    pool.apply_async(worker, (model,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5c221-7250-4ead-b17e-3f7a4262b3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "killall_pods(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f79168-309e-420d-bcad-2c49197d443b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
